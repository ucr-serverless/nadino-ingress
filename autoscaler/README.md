# Palladium Ingress HPA (Horizontal Process Autoscaler)

## Build F-stack's top
We use the `top` tool from F-stack to collect the CPU usage of F-stack (reported as `sys`) and
NGINX (reported as `usr`).

`hpa.py` consumes `stdout` from the top tool to collect the CPU usage.
It's necessary to call `fflush(stdout)` after each output to ensure that
the contents of the buffer are written to standard output immediately.

We provide a customized F-stack `top` with `fflush(stdout)` enforced (in the current directory).
Please do not use the  default F-stack `top`.

```bash
# Copy top.c to designated directory
cp ~/palladium-ingress/autoscaler/top.c ~/palladium-ingress/f-stack/tools/top/

# Compile F-stack top
cd ~/palladium-ingress/f-stack/tools/
make -j
```

## Configure hpa.py
There are several configurable parameters in `hpa.py`:
- `NGINX_CONF_PATH`: Path to NGINX config file. Default is `/usr/local/nginx_fstack/conf/nginx.conf`
- `FSTACK_CONF_PATH`: Path to F-stack config file. Default is `/usr/local/nginx_fstack/conf/f-stack.conf`
- `TOP_COMMAND`: Path to the binary of F-stack top. Default is `/users/sqi009/palladium-ingress/f-stack/tools/sbin/top`
- `NGINX_RELOAD_CMD`: Command to reload NGINX. Default is `sudo /usr/local/nginx_fstack/sbin/nginx -s reload`
- `EWMA_ALPHA`: Smoothing factor of EWMA.
- `MAX_WORKERS`: Maximum number of worker processes allowed
- `SCALE_UP_THRESHOLD` and `SCALE_DOWN_THRESHOLD`: Scaling thresholds
- `DECISION_INTERVAL`: Interval for autoscaling-making

## Run Ingress HPA
```bash
cd ~/palladium-ingress/autoscaler
python hpa.py
```

## Load generator (wrk)
We provide a Lua script (`traffic.lua`) that records the request rate of the current wrk instance every second. 
Use the following command to run `wrk` and load the Lua script:
- `-t1`: Use 1 thread.
- `-c12`: 12 concurrent connections (enough for a single thread).
- `-d30s`: test lasts 30 seconds.
- `-s traffic.lua`: Load the Lua script.
- `-- 1`: specify the wrk client ID (for result collection) 

```bash
wrk -t1 -c12 -d30s -s traffic.lua http://<Cluster-Ingress-IP>/<location>/ -- 1
```

## Dynamic workload generation
To generate dynamic workload, we provide a bash script `dynamic_traffic.sh`.
During the experiment, `dynamic_traffic.sh` gradually increases the number of `wrk` instances: At the beginning there is only one `wrk` instance, and every 10 seconds the script adds one `wrk` instance. After adding 10 `wrk` instances, we run these 10 instances for 30 seconds. Then we reduce one `wrk` instance every 10 seconds. After reducing the number of instances to 5, we increase the number of instances again every 10 seconds. We run a maximum of 15 instances until the end of the experiment. Note that each `wrk` instance only runs one thread and 12 connections.
Each `wrk` instance writes the collected request rate to its own log file `wrk_clt_$ID_traffic_log.csv`.

```bash
./dynamic_traffic.sh http://<Cluster-Ingress-IP>/<location>/
```

You can customize `dynamic_traffic.sh` to change the behavior of the workload.

## Process collected metrics
`aggregate_logs.py` is used for aggregating the request rate of different wrk instances under the same wall-clock time and generate an aggregated log file (`aggregated_traffic_log.csv`).
This helps analyze the total request_rate generated by all the wrk instances at each second.

```bash
python aggregate_logs.py
```

## Notes
Palladium Cluster Ingress currently does not support graceful reloading, meaning that the Cluster Ingress is unavailable and causes service disruption during the autoscaling procedure.

We recommend using a load generator that supports TCP connection re-establishment (e.g. `wrk`) for experiments.

Load generators such as Apache Benchmark terminate after the connection reset and are therefore not recommended.
